{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNgI0rXksDC"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C1oH8jeWksDC"
      },
      "outputs": [],
      "source": [
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# for visualizing the results\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for reading input data\n",
        "import pandas as pd\n",
        "\n",
        "# for parsing the FEN of chess positions\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mX98WoqksDD"
      },
      "source": [
        "To represent a chess position, it is common to use [Forsythâ€“Edwards Notation (FEN)](http://https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation) which contains all the necessary information to reconstruct a chess game from the current position. To make this information usable for a neural network, we will use a bit (actually a byte) to represent if a specific piece (white rook, white knight, etc...) is on a specific square on the 8x8 chess board. Since there are 6 different pieces and two different players, that means there are 12 specific pieces that could potentially be on each square.\n",
        "\n",
        "However, we still need to keep track of information like whose turn it is, which castling options are still legal, if en passant is possible, how many half moves since a pawn move or piece capture, and how many turns the game has had. To do this we use an additional 8x8 board where the rook locations represent castling rights, the 3rd and 6th rank (row) keep track of possible en passant moves, the e1 and e8 sqaure represent whose on move, and the 4th and 5th rank represent the number of half moves and full moves as binary numbers (max possible being 255) respectively.\n",
        "\n",
        "Below is a function to do this conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BGZIEyOnksDD"
      },
      "outputs": [],
      "source": [
        "def fen_to_bit_vector(fen):\n",
        "    # piece placement - lowercase for black pieces, uppercase for white pieces. numbers represent consequtive spaces. / represents a new row\n",
        "    # active color - whose turn it is, either 'w' or 'b'\n",
        "    # castling rights - which castling moves are still legal K or k for kingside and Q or q for queenside, '-' if no legal castling moves for either player\n",
        "    # en passant - if the last move was a pawn moving up two squares, this is the space behind the square for the purposes of en passant\n",
        "    # halfmove clock - number of moves without a pawn move or piece capture, after 50 of which the game is a draw\n",
        "    # fullmove number - number of full turns starting at 1, increments after black's move\n",
        "\n",
        "    # Example FEN of starting position\n",
        "    # rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
        "\n",
        "    parts = re.split(\" \", fen)\n",
        "    piece_placement = re.split(\"/\", parts[0])\n",
        "    active_color = parts[1]\n",
        "    castling_rights = parts[2]\n",
        "    en_passant = parts[3]\n",
        "    halfmove_clock = int(parts[4])\n",
        "    fullmove_clock = int(parts[5])\n",
        "\n",
        "    bit_vector = np.zeros((13, 8, 8), dtype=np.uint8)\n",
        "\n",
        "    # piece to layer structure taken from reference [1]\n",
        "    piece_to_layer = {\n",
        "        'R': 1,\n",
        "        'N': 2,\n",
        "        'B': 3,\n",
        "        'Q': 4,\n",
        "        'K': 5,\n",
        "        'P': 6,\n",
        "        'p': 7,\n",
        "        'k': 8,\n",
        "        'q': 9,\n",
        "        'b': 10,\n",
        "        'n': 11,\n",
        "        'r': 12\n",
        "    }\n",
        "\n",
        "    castling = {\n",
        "        'K': (7,7),\n",
        "        'Q': (7,0),\n",
        "        'k': (0,7),\n",
        "        'q': (0,0),\n",
        "    }\n",
        "\n",
        "    for r, row in enumerate(piece_placement):\n",
        "        c = 0\n",
        "        for piece in row:\n",
        "            if piece in piece_to_layer:\n",
        "                bit_vector[piece_to_layer[piece], r, c] = 1\n",
        "                c += 1\n",
        "            else:\n",
        "                c += int(piece)\n",
        "\n",
        "    if en_passant != '-':\n",
        "        bit_vector[0, ord(en_passant[0]) - ord('a'), int(en_passant[1]) - 1] = 1\n",
        "\n",
        "    if castling_rights != '-':\n",
        "        for char in castling_rights:\n",
        "            bit_vector[0, castling[char][0], castling[char][1]] = 1\n",
        "\n",
        "    if active_color == 'w':\n",
        "        bit_vector[0, 7, 4] = 1\n",
        "    else:\n",
        "        bit_vector[0, 0, 4] = 1\n",
        "\n",
        "    if halfmove_clock > 0:\n",
        "        c = 7\n",
        "        while halfmove_clock > 0:\n",
        "            bit_vector[0, 3, c] = halfmove_clock%2\n",
        "            halfmove_clock = halfmove_clock // 2\n",
        "            c -= 1\n",
        "            if c < 0:\n",
        "                break\n",
        "\n",
        "    if fullmove_clock > 0:\n",
        "        c = 7\n",
        "        while fullmove_clock > 0:\n",
        "            bit_vector[0, 4, c] = fullmove_clock%2\n",
        "            fullmove_clock = fullmove_clock // 2\n",
        "            c -= 1\n",
        "            if c < 0:\n",
        "                break\n",
        "\n",
        "    return bit_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBYxTFVwksDD",
        "outputId": "749159d3-1273-4aed-ab29-3093c6dc9f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[1 0 0 0 0 0 0 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 1 0 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 0 0 0 1 0 0 1]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 0 0 0 0 0 0 1]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 1 0 0 0 0 1 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 1 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 1 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 1 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0]\n",
            "  [1 1 1 1 1 1 1 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0 1 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 1 0 0 1 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 1 0 0 0 0 1 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[1 0 0 0 0 0 0 1]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0]]]\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n",
            "1000000100000000000000000000010100000000000000000000000010001001000000000000000000000000000000000000000000000000000000001000000100000000000000000000000000000000000000000000000000000000010000100000000000000000000000000000000000000000000000000000000000100100000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000000010000000000000000000000000000000000000000000000000001111111100000000000000001111111100000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000001000000000000000000000000000000000000000000000000000000000000001001000000000000000000000000000000000000000000000000000000000001000010000000000000000000000000000000000000000000000000000000001000000100000000000000000000000000000000000000000000000000000000\n"
          ]
        }
      ],
      "source": [
        "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 5 0\"\n",
        "board = fen_to_bit_vector(fen)\n",
        "print(board)\n",
        "\n",
        "fentensor = torch.flatten(torch.from_numpy(board))\n",
        "print(fentensor)\n",
        "\n",
        "fenstring = ''.join(map(str,fentensor.numpy().tolist()))\n",
        "print(fenstring)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCiv898dksDD"
      },
      "source": [
        "The first 8x8 board (0th index) contains all the extra information and the following 12 boards (1 to 12) represent the locations of the pieces in the order\n",
        "\n",
        "1. White Rook\n",
        "2. White Knight\n",
        "3. White Bishop\n",
        "4. White Queen\n",
        "5. White King\n",
        "6. White Pawn\n",
        "7. Black Pawn\n",
        "8. Black King\n",
        "9. Black Queen\n",
        "10. Black Bishop\n",
        "11. Black Knight\n",
        "12. Black Rook\n",
        "\n",
        "Notice how the pieces line up correctly with the starting position with the first board correctly indicating it is white to move."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNiqLhP2ksDE"
      },
      "source": [
        "# Neural Network\n",
        "\n",
        "We'll begin with a simple Feed-Forward Neural Network that's fully connected. Neural Networks are named for their structure being analogous to neurons in the human brain. The idea is that, in the human brain, when a neuron gets an electrical impulse through its synapses it will sometimes fire an electrical impulse to other neurons, creating a chain reaction. For neural networks, our neurons are nodes our synapses are edges (with corresponding weights) and the firing of the neuron is the activation function and output of the node.\n",
        "\n",
        "![Perceptron](http://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg)\n",
        "\n",
        "The goal of the Neural Network is to have weights such that after all the chain reactions of nodes taking in inputs and producing outputs, the information output of the final node represents the evaluation of the chess position that began the process. In order to actually find such weights we will use a method known as backpropagation, which iteratively adjusts the weights in the network to nudge the output closer to the answer we desire.\n",
        "\n",
        "Technically speaking, for each training record (a FEN and an evaulation) we input the position into the Neural Network, and after we get a result we compute the error between the result and the correct evaluation which can be represented as a error function. To change the weights in such a way as to minimize this error function we compute the gradient of the error function and adjust the weights in the opposite direction. This means that if we overshoot we want to decrease our evaluation and if we undershoot we want to increase our evaluation.\n",
        "\n",
        "![Gradient Descent](https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2x6pjt3TksDE"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(832, 832)\n",
        "        self.fc2 = nn.Linear(832, 832)\n",
        "        self.fc3 = nn.Linear(832, 832)\n",
        "        self.fc4 = nn.Linear(832, 832)\n",
        "        self.fc5 = nn.Linear(832, 832)\n",
        "        self.fc6 = nn.Linear(832, 832)\n",
        "        self.fc7 = nn.Linear(832, 832)\n",
        "        self.fc8 = nn.Linear(832, 832)\n",
        "        self.fc9 = nn.Linear(832, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        x = F.relu(self.fc8(x))\n",
        "        x = self.fc9(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3R4OjltkksDE"
      },
      "outputs": [],
      "source": [
        "# ChessDataset code and eval_to_int code taken from reference [1]\n",
        "class ChessDataset(Dataset):\n",
        "    def __init__(self, data_frame):\n",
        "        self.fens = torch.from_numpy(np.array([*map(fen_to_bit_vector, data_frame[\"FEN\"])], dtype=np.float32))\n",
        "        self.evals = torch.Tensor([[x] for x in data_frame[\"Evaluation\"]])\n",
        "        self._len = len(self.evals)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.fens[index], self.evals[index]\n",
        "\n",
        "\n",
        "def eval_to_int(evaluation):\n",
        "    try:\n",
        "        res = int(evaluation)\n",
        "    except ValueError:\n",
        "        res = 10000 if evaluation[1] == '+' else -10000\n",
        "    return res / 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F1nC3QyrksDE"
      },
      "outputs": [],
      "source": [
        "def AdamW_main():\n",
        "    MAX_DATA = 5000000\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "    print(\"Using device {}\".format(device))\n",
        "\n",
        "    print(\"Preparing Training Data...\")\n",
        "    train_data = pd.read_csv(\"kaggle/input/chess-evaluations/chessData.csv\")\n",
        "    train_data = train_data[:MAX_DATA]\n",
        "    train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n",
        "    trainset = ChessDataset(train_data)\n",
        "\n",
        "    print(\"Preparing Test Data...\")\n",
        "    test_data = pd.read_csv(\"kaggle/input/chess-evaluations/tactic_evals.csv\")\n",
        "    test_data = test_data[:MAX_DATA]\n",
        "    test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n",
        "    testset = ChessDataset(test_data)\n",
        "\n",
        "    batch_size = 1024\n",
        "\n",
        "    print(\"Converting to pytorch Dataset...\")\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(\"Trainset loaded\")\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(\"Testset loaded\")\n",
        "\n",
        "    net = Net().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(net.parameters())\n",
        "\n",
        "    print(\"Training Net\")\n",
        "\n",
        "    for epoch in range(100):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, len(trainloader), running_loss / len(trainloader)))\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    PATH = './chessv5.pth'\n",
        "    torch.save(net.state_dict(), PATH)\n",
        "\n",
        "    weights_output = []\n",
        "\n",
        "    for name, param in net.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            print(f\"Layer: {name}, Shape: {param.size()}\")\n",
        "            weights_output.append(param.detach().cpu().numpy())\n",
        "\n",
        "    out = np.vstack(weights_output)\n",
        "    print(out.shape)\n",
        "\n",
        "    np.savetxt('nnweightsv5.csv', out, delimiter=',', fmt='%f')\n",
        "\n",
        "    print('Evaluating model')\n",
        "\n",
        "    count = 0\n",
        "    total_loss = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n",
        "\n",
        "            # count should represent the number of positions evaluated\n",
        "            # independent of the batch size\n",
        "            count += len(labels)\n",
        "            total_loss += loss\n",
        "            if count % 10000 == 0:\n",
        "                print('Average error of the model on the {} tactics positions is {}'.format(count, total_loss/count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "MvX8kjyWksDE",
        "outputId": "ae2d0a6c-d07c-45a7-e061-ac67c6e39881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device mps\n",
            "Preparing Training Data...\n",
            "Preparing Test Data...\n",
            "Converting to pytorch Dataset...\n",
            "Trainset loaded\n",
            "Testset loaded\n",
            "Training Net\n",
            "Epoch:\n",
            "1\n",
            "[1,  4883] loss: 119.869\n",
            "Epoch:\n",
            "2\n",
            "[2,  4883] loss: 77.602\n",
            "Epoch:\n",
            "3\n",
            "[3,  4883] loss: 61.705\n",
            "Epoch:\n",
            "4\n",
            "[4,  4883] loss: 52.809\n",
            "Epoch:\n",
            "5\n",
            "[5,  4883] loss: 46.621\n",
            "Epoch:\n",
            "6\n",
            "[6,  4883] loss: 42.203\n",
            "Epoch:\n",
            "7\n",
            "[7,  4883] loss: 38.969\n",
            "Epoch:\n",
            "8\n",
            "[8,  4883] loss: 36.125\n",
            "Epoch:\n",
            "9\n",
            "[9,  4883] loss: 33.936\n",
            "Epoch:\n",
            "10\n",
            "[10,  4883] loss: 32.129\n",
            "Epoch:\n",
            "11\n",
            "[11,  4883] loss: 30.645\n",
            "Epoch:\n",
            "12\n",
            "[12,  4883] loss: 29.349\n",
            "Epoch:\n",
            "13\n",
            "[13,  4883] loss: 28.072\n",
            "Epoch:\n",
            "14\n",
            "[14,  4883] loss: 27.128\n",
            "Epoch:\n",
            "15\n",
            "[15,  4883] loss: 26.290\n",
            "Epoch:\n",
            "16\n",
            "[16,  4883] loss: 25.573\n",
            "Epoch:\n",
            "17\n",
            "[17,  4883] loss: 24.797\n",
            "Epoch:\n",
            "18\n",
            "[18,  4883] loss: 24.272\n",
            "Epoch:\n",
            "19\n",
            "[19,  4883] loss: 23.779\n",
            "Epoch:\n",
            "20\n",
            "[20,  4883] loss: 23.455\n",
            "Epoch:\n",
            "21\n",
            "[21,  4883] loss: 22.836\n",
            "Epoch:\n",
            "22\n",
            "[22,  4883] loss: 22.522\n",
            "Epoch:\n",
            "23\n",
            "[23,  4883] loss: 22.218\n",
            "Epoch:\n",
            "24\n",
            "[24,  4883] loss: 21.975\n",
            "Epoch:\n",
            "25\n",
            "[25,  4883] loss: 21.773\n",
            "Epoch:\n",
            "26\n",
            "[26,  4883] loss: 21.575\n",
            "Epoch:\n",
            "27\n",
            "[27,  4883] loss: 21.350\n",
            "Epoch:\n",
            "28\n",
            "[28,  4883] loss: 21.348\n",
            "Epoch:\n",
            "29\n",
            "[29,  4883] loss: 20.987\n",
            "Epoch:\n",
            "30\n",
            "[30,  4883] loss: 20.873\n",
            "Epoch:\n",
            "31\n",
            "[31,  4883] loss: 20.622\n",
            "Epoch:\n",
            "32\n",
            "[32,  4883] loss: 20.811\n",
            "Epoch:\n",
            "33\n",
            "[33,  4883] loss: 20.393\n",
            "Epoch:\n",
            "34\n",
            "[34,  4883] loss: 20.235\n",
            "Epoch:\n",
            "35\n",
            "[35,  4883] loss: 20.123\n",
            "Epoch:\n",
            "36\n",
            "[36,  4883] loss: 19.953\n",
            "Epoch:\n",
            "37\n",
            "[37,  4883] loss: 19.676\n",
            "Epoch:\n",
            "38\n",
            "[38,  4883] loss: 19.744\n",
            "Epoch:\n",
            "39\n",
            "[39,  4883] loss: 19.507\n",
            "Epoch:\n",
            "40\n",
            "[40,  4883] loss: 19.451\n",
            "Epoch:\n",
            "41\n",
            "[41,  4883] loss: 19.447\n",
            "Epoch:\n",
            "42\n",
            "[42,  4883] loss: 19.338\n",
            "Epoch:\n",
            "43\n",
            "[43,  4883] loss: 19.141\n",
            "Epoch:\n",
            "44\n",
            "[44,  4883] loss: 19.152\n",
            "Epoch:\n",
            "45\n",
            "[45,  4883] loss: 19.159\n",
            "Epoch:\n",
            "46\n",
            "[46,  4883] loss: 19.104\n",
            "Epoch:\n",
            "47\n",
            "[47,  4883] loss: 18.889\n",
            "Epoch:\n",
            "48\n",
            "[48,  4883] loss: 18.816\n",
            "Epoch:\n",
            "49\n",
            "[49,  4883] loss: 18.805\n",
            "Epoch:\n",
            "50\n",
            "[50,  4883] loss: 18.737\n",
            "Epoch:\n",
            "51\n",
            "[51,  4883] loss: 18.469\n",
            "Epoch:\n",
            "52\n",
            "[52,  4883] loss: 18.666\n",
            "Epoch:\n",
            "53\n",
            "[53,  4883] loss: 18.543\n",
            "Epoch:\n",
            "54\n",
            "[54,  4883] loss: 18.521\n",
            "Epoch:\n",
            "55\n",
            "[55,  4883] loss: 18.351\n",
            "Epoch:\n",
            "56\n",
            "[56,  4883] loss: 18.541\n",
            "Epoch:\n",
            "57\n",
            "[57,  4883] loss: 18.399\n",
            "Epoch:\n",
            "58\n",
            "[58,  4883] loss: 18.244\n",
            "Epoch:\n",
            "59\n",
            "[59,  4883] loss: 18.209\n",
            "Epoch:\n",
            "60\n",
            "[60,  4883] loss: 18.294\n",
            "Epoch:\n",
            "61\n",
            "[61,  4883] loss: 18.286\n",
            "Epoch:\n",
            "62\n",
            "[62,  4883] loss: 18.196\n",
            "Epoch:\n",
            "63\n",
            "[63,  4883] loss: 18.231\n",
            "Epoch:\n",
            "64\n",
            "[64,  4883] loss: 18.390\n",
            "Epoch:\n",
            "65\n",
            "[65,  4883] loss: 18.210\n",
            "Epoch:\n",
            "66\n",
            "[66,  4883] loss: 18.061\n",
            "Epoch:\n",
            "67\n",
            "[67,  4883] loss: 18.144\n",
            "Epoch:\n",
            "68\n",
            "[68,  4883] loss: 18.053\n",
            "Epoch:\n",
            "69\n",
            "[69,  4883] loss: 18.086\n",
            "Epoch:\n",
            "70\n",
            "[70,  4883] loss: 17.940\n",
            "Epoch:\n",
            "71\n",
            "[71,  4883] loss: 18.077\n",
            "Epoch:\n",
            "72\n",
            "[72,  4883] loss: 18.116\n",
            "Epoch:\n",
            "73\n",
            "[73,  4883] loss: 17.889\n",
            "Epoch:\n",
            "74\n",
            "[74,  4883] loss: 18.019\n",
            "Epoch:\n",
            "75\n",
            "[75,  4883] loss: 17.836\n",
            "Epoch:\n",
            "76\n",
            "[76,  4883] loss: 18.028\n",
            "Epoch:\n",
            "77\n",
            "[77,  4883] loss: 18.009\n",
            "Epoch:\n",
            "78\n",
            "[78,  4883] loss: 17.832\n",
            "Epoch:\n",
            "79\n",
            "[79,  4883] loss: 17.908\n",
            "Epoch:\n",
            "80\n",
            "[80,  4883] loss: 17.845\n",
            "Epoch:\n",
            "81\n",
            "[81,  4883] loss: 17.882\n",
            "Epoch:\n",
            "82\n",
            "[82,  4883] loss: 17.826\n",
            "Epoch:\n",
            "83\n",
            "[83,  4883] loss: 17.837\n",
            "Epoch:\n",
            "84\n",
            "[84,  4883] loss: 17.831\n",
            "Epoch:\n",
            "85\n",
            "[85,  4883] loss: 17.951\n",
            "Epoch:\n",
            "86\n",
            "[86,  4883] loss: 17.691\n",
            "Epoch:\n",
            "87\n",
            "[87,  4883] loss: 17.835\n",
            "Epoch:\n",
            "88\n",
            "[88,  4883] loss: 18.011\n",
            "Epoch:\n",
            "89\n",
            "[89,  4883] loss: 17.607\n",
            "Epoch:\n",
            "90\n",
            "[90,  4883] loss: 17.749\n",
            "Epoch:\n",
            "91\n",
            "[91,  4883] loss: 17.847\n",
            "Epoch:\n",
            "92\n",
            "[92,  4883] loss: 17.593\n",
            "Epoch:\n",
            "93\n",
            "[93,  4883] loss: 17.625\n",
            "Epoch:\n",
            "94\n",
            "[94,  4883] loss: 17.749\n",
            "Epoch:\n",
            "95\n",
            "[95,  4883] loss: 17.742\n",
            "Epoch:\n",
            "96\n",
            "[96,  4883] loss: 17.522\n",
            "Epoch:\n",
            "97\n",
            "[97,  4883] loss: 17.608\n",
            "Epoch:\n",
            "98\n",
            "[98,  4883] loss: 17.648\n",
            "Epoch:\n",
            "99\n",
            "[99,  4883] loss: 17.648\n",
            "Epoch:\n",
            "100\n",
            "[100,  4883] loss: 17.495\n",
            "Finished Training\n",
            "Layer: fc1.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc2.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc3.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc4.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc5.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc6.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc7.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc8.weight, Shape: torch.Size([832, 832])\n",
            "Layer: fc9.weight, Shape: torch.Size([1, 832])\n",
            "(6657, 832)\n",
            "Evaluating model\n",
            "Average error of the model on the 640000 tactics positions is 2.269597053527832\n",
            "Average error of the model on the 1280000 tactics positions is 1.8036402463912964\n",
            "Average error of the model on the 1920000 tactics positions is 1.487705945968628\n",
            "Average error of the model on the 2560000 tactics positions is 1.2599107027053833\n"
          ]
        }
      ],
      "source": [
        "AdamW_main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPUHKTwckKRx"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 643409,
          "sourceId": 2345997,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30152,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
